#!/usr/bin/env python3
# -*- mode: python -*-
# =============================================================================
#  @@-COPYRIGHT-START-@@
#
#  Copyright (c) 2023 of Qualcomm Innovation Center, Inc. All rights reserved.
#
#  @@-COPYRIGHT-END-@@
# =============================================================================
#pylint: skip-file
""" module for getting dataloders"""
import os 
import logging
import math
import pathlib
from itertools import chain
import numpy as np
from datasets import load_dataset

import torch
from torch.utils.data import DataLoader

from transformers import (AutoTokenizer,
    default_data_collator)


class DataConfig:
    """adding hardcoded values into args from parseargs() and return config object"""

    def __init__(self, args):
        self.dataset_name = "wikitext"
        self.dataset_config_name = "wikitext-2-raw-v1"
        self.train_file = None
        self.validation_file = None
        self.validation_split_percentage = 5
        self.parent_dir = str(pathlib.Path(os.path.abspath(__file__)).parent.parent)
        self.model_name_or_path = os.path.join(self.parent_dir,"model/weights/downloaded_weights")
        self.use_slow_tokenizer = False
        self.per_device_train_batch_size = 4
        self.block_size = 256
        self.preprocessing_num_workers = None
        self.overwrite_cache = False
        for arg in vars(args):
            setattr(self, arg, getattr(args, arg))



def get_dataloaders(args):
    """Get the dataloaders"""
    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)
    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/
    # (the dataset will be downloaded automatically from the datasets Hub).
    #
    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called
    # 'text' is found. You can easily tweak this behavior (see below).
    #
    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.

    # hardcode arguments 
    args = DataConfig(args)
    if args.dataset_name is not None:
        # Downloading and loading a dataset from the hub.
        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
        if "validation" not in raw_datasets.keys():
            raw_datasets["validation"] = load_dataset(
                args.dataset_name,
                args.dataset_config_name,
                split=f"train[:{args.validation_split_percentage}%]",
            )
            raw_datasets["train"] = load_dataset(
                args.dataset_name,
                args.dataset_config_name,
                split=f"train[{args.validation_split_percentage}%:]",
            )
    else:
        data_files = {}
        dataset_args = {}
        if args.train_file is not None:
            data_files["train"] = args.train_file
        if args.validation_file is not None:
            data_files["validation"] = args.validation_file
        extension = args.train_file.split(".")[-1]
        if extension == "txt":
            extension = "text"
            dataset_args["keep_linebreaks"] = not args.no_keep_linebreaks
        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)
        # If no validation data is there, validation_split_percentage will be used to divide the dataset.
        if "validation" not in raw_datasets.keys():
            raw_datasets["validation"] = load_dataset(
                extension,
                data_files=data_files,
                split=f"train[:{args.validation_split_percentage}%]",
                **dataset_args,
            )
            raw_datasets["train"] = load_dataset(
                extension,
                data_files=data_files,
                split=f"train[{args.validation_split_percentage}%:]",
                **dataset_args,
            )
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)

    # Preprocessing the datasets.
    # First we tokenize all the texts.
    column_names = raw_datasets["train"].column_names
    text_column_name = "text" if "text" in column_names else column_names[0]

    def tokenize_function(examples):
        return tokenizer(examples[text_column_name])


    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,
        num_proc=args.preprocessing_num_workers,
        remove_columns=column_names,
        load_from_cache_file=not args.overwrite_cache,
        desc="Running tokenizer on dataset",
    )

    if args.block_size > tokenizer.model_max_length:
        logging.warning(
            f"The block_size passed ({args.block_size}) is larger than the maximum length for the model"
            f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
    block_size = min(args.block_size, tokenizer.model_max_length)

    def group_texts(examples):
        """"Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size"""

        # Concatenate all texts.
        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
        # customize this part to your needs.
        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
        # Split by chunks of max_len.
        result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result

    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder
    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower
    # to preprocess.
    #
 
    lm_datasets = tokenized_datasets.map(
        group_texts,
        batched=True,
        num_proc=args.preprocessing_num_workers,
        load_from_cache_file=not args.overwrite_cache,
        desc=f"Grouping texts in chunks of {block_size}",
    )

    train_dataset = lm_datasets["train"]
    eval_dataset = lm_datasets["validation"]

    # DataLoaders creation:
    train_dataloader = DataLoader(
        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size
    )
    eval_dataloader = DataLoader(
        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size
    )


    def eval_function(model,args):
        """evaluation function"""
        iterations=args[0]
        loader=args[1]
        metric=args[2]
        model.eval()
        losses = []
        for step, batch in enumerate(loader):
            if step < iterations:
                for k in batch.keys():
                    batch[k] = batch[k].to('cuda')
                with torch.no_grad():
                    outputs = model(**batch)
                losses.append(outputs[0].item())
            else:
                break
        loss = np.mean(losses)
        if metric == "loss":
            return loss
        elif metric == "perplexity":
            try:
                perplexity = math.exp(loss)
            except OverflowError:
                perplexity = float('inf')
            return perplexity
        else:
            raise ValueError("invalid metric: ", metric) 


    return train_dataloader,eval_dataloader,eval_function
